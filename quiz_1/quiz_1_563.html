<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>quiz_1_563</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="quiz_1_563_files/libs/clipboard/clipboard.min.js"></script>
<script src="quiz_1_563_files/libs/quarto-html/quarto.js"></script>
<script src="quiz_1_563_files/libs/quarto-html/popper.min.js"></script>
<script src="quiz_1_563_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="quiz_1_563_files/libs/quarto-html/anchor.min.js"></script>
<link href="quiz_1_563_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="quiz_1_563_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="quiz_1_563_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="quiz_1_563_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="quiz_1_563_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="k-means-clustering" class="level2">
<h2 class="anchored" data-anchor-id="k-means-clustering">K-Means Clustering</h2>
<ul>
<li><strong>Clustering</strong>: Task of partitioning data into groups called clusters based on similarity</li>
</ul>
<p><strong>Comparisons with kNN</strong>:</p>
<table class="table">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>k-Means</th>
<th>k-Nearest Neighbors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Unsupervised</td>
<td>Supervised</td>
</tr>
<tr class="even">
<td>Clustering</td>
<td>Classification</td>
</tr>
<tr class="odd">
<td>Parametric</td>
<td>Non-parametric</td>
</tr>
<tr class="even">
<td>k = number of clusters (high = overfitting)</td>
<td>k = number of neighbors (low = overfitting)</td>
</tr>
</tbody>
</table>
<section id="common-applications" class="level4">
<h4 class="anchored" data-anchor-id="common-applications">Common Applications</h4>
<ol type="1">
<li>Data Exploration: Summarize/ compress data, Partition data into groups</li>
<li>Customer Segmentation: Group customers based on purchasing behavior, Target marketing</li>
<li>Document Clustering: Group similar documents together, Topic modeling</li>
</ol>
</section>
<section id="k-means" class="level3">
<h3 class="anchored" data-anchor-id="k-means">K-Means</h3>
<ul>
<li>One of the most popular clustering algorithms</li>
<li>Simple and easy to implement</li>
<li><strong>Basic Algorithm</strong>:
<ol type="1">
<li>Randomly initialize <span class="math inline">\(K\)</span> centroids</li>
<li>Assign each data point to the nearest centroid</li>
<li>Update the centroids to the mean of the data points assigned to it</li>
<li>Repeat steps 2 and 3 until convergence</li>
</ol></li>
<li><strong>Properties</strong>:
<ul>
<li>Will always converge (not necessarily to the right answer)</li>
<li>Sensitive to intialization</li>
<li>Terminates when centroids do not change</li>
<li>Makes <em>linear</em> decision boundaries</li>
<li><strong>MUST SCALE DATA</strong> before applying K-means
<ul>
<li>Because K-means uses distance</li>
<li>If features are on different scales, the clustering will be biased towards the features with larger scales</li>
</ul></li>
</ul></li>
</ul>
<hr>
<ul>
<li><strong>Input</strong>:
<ul>
<li><span class="math inline">\(X\)</span>: Set of <span class="math inline">\(n\)</span> data points</li>
<li><span class="math inline">\(K\)</span>: Number of clusters</li>
</ul></li>
<li><strong>Output</strong>:
<ul>
<li><span class="math inline">\(K\)</span> clusters with centroids <span class="math inline">\(\mu_1, \mu_2, \ldots, \mu_K\)</span></li>
</ul></li>
</ul>
<section id="choosing-k" class="level4">
<h4 class="anchored" data-anchor-id="choosing-k">Choosing K</h4>
<ul>
<li>K is a hyperparameter
<ul>
<li>As K increases -&gt; smaller clusters</li>
</ul></li>
<li>No perfect way to choose K</li>
</ul>
<ol type="1">
<li><p><strong>Elbow Method</strong>:</p>
<ul>
<li>Specific to k-means</li>
<li><span class="math inline">\(\text{Inertia} = \text{sum of intra-cluster distances}\)</span>
<ul>
<li>Sum of centroid to point distances of all points in the cluster of all clusters <span class="math inline">\(\sum_{i=1}^{K} \sum_{x \in C_i} ||x - \mu_i||^2\)</span></li>
</ul></li>
<li>Inertia decreases as K increases, until it reaches 0 when K = n</li>
<li>Plot inertia vs K
<ul>
<li>Elbow point: point where inertia starts to decrease more slowly</li>
</ul></li>
</ul></li>
<li><p><strong>Silhouette Score</strong>:</p>
<ul>
<li><p><strong>Not dependent on cluster centers</strong> -&gt; can be used to compare different clustering algorithms</p></li>
<li><p>Gets worst as K increases, since being closer to neigouring clusters <span class="math display">\[\text{Silhouette Score} = \frac{b - a}{\max(a, b)}\]</span></p></li>
<li><p><span class="math inline">\(a\)</span>: Mean distance between a sample and all other points in the same cluster</p></li>
<li><p><span class="math inline">\(b\)</span>: Mean distance between a sample and all other points in the next nearest cluster</p></li>
<li><p>Range: <span class="math inline">\([-1, 1]\)</span> (higher is better = high correlation within cluster)</p></li>
<li><p><strong>y-axis</strong>: Sample number (similar thickness = balanced cluster sizes)</p></li>
<li><p><strong>x-axis</strong>: Silhouette score</p></li>
</ul></li>
</ol>
</section>
</section>
</section>
<section id="gaussian-mixture-models-high-level-overview" class="level2">
<h2 class="anchored" data-anchor-id="gaussian-mixture-models-high-level-overview">Gaussian Mixture Models (High-Level Overview)</h2>
<ul>
<li><code>GaussianMixture(n_components=3, covariance_type='full')</code>, <code>covariance_type</code>:
<ul>
<li><code>full</code>: Each component has its own general covariance matrix
<ul>
<li>size: <span class="math inline">\(K \times n\_features \times n\_features\)</span></li>
</ul></li>
<li><code>tied</code>: All components share the same general covariance matrix
<ul>
<li>size: <span class="math inline">\(n\_features \times n\_features\)</span></li>
</ul></li>
<li><code>diag</code>: Each component has its own diagonal covariance matrix
<ul>
<li>size: <span class="math inline">\(K \times n\_features\)</span></li>
</ul></li>
<li><code>spherical</code>: Each component has its own single variance
<ul>
<li>size: <span class="math inline">\(K\)</span></li>
</ul></li>
</ul></li>
</ul>
<section id="how-gmms-work" class="level3">
<h3 class="anchored" data-anchor-id="how-gmms-work">How GMMs Work</h3>
<p><span class="math display">\[P(x) = \sum_{k=1}^{K} \pi_k \mathcal{N}(x | \mu_k, \Sigma_k)\]</span></p>
<ul>
<li><p><span class="math inline">\(P(x)\)</span>: Probability of observing <span class="math inline">\(x\)</span></p></li>
<li><p><span class="math inline">\(\pi_k\)</span>: Weight of the <span class="math inline">\(k\)</span>th Gaussian (between 0 and 1)</p></li>
<li><p><span class="math inline">\(k\)</span>: Number of clusters</p></li>
<li><p><span class="math inline">\(\mathcal{N}(x | \mu_k, \Sigma_k)\)</span>: Gaussian distribution with mean <span class="math inline">\(\mu_k\)</span> and covariance <span class="math inline">\(\Sigma_k\)</span></p></li>
<li><p><strong>Generative Model</strong>: models the probability of a data point being generated from the mixture of Gaussians</p></li>
<li><p><strong>High Level Algorithm</strong> (non-convex optimization):</p>
<ol type="1">
<li>Initialize <span class="math inline">\(\pi_k, \mu_k, \Sigma_k\)</span> (sensitive to init, can init with k-means)</li>
<li>E-step: Each point, Compute the probability of each data point belonging to each cluster</li>
<li>M-step: Each cluster, Update <span class="math inline">\(\pi_k, \mu_k, \Sigma_k\)</span> to maximize the likelihood of the data</li>
<li>Repeat steps 2 and 3 until convergence</li>
</ol></li>
<li><p>Under the hood, GMMs use the <strong>Expectation-Maximization (EM)</strong> algorithm.</p>
<ul>
<li>Basic idea: treat cluster assignments as hidden variables and iteratively update them</li>
</ul></li>
<li><p><strong>Other Properties</strong>:</p>
<ul>
<li>Can constrain the covariance matrix</li>
<li>Number of clusters is a hyperparameter and has a significant impact on the model</li>
</ul></li>
</ul>
</section>
</section>
<section id="dbscan" class="level2">
<h2 class="anchored" data-anchor-id="dbscan">DBSCAN</h2>
<ul>
<li><p>Density-Based Spatial Clustering of Applications with Noise</p></li>
<li><p><strong>Idea</strong>: Clusters are dense regions in the data space, separated by low-density regions</p></li>
<li><p>Addresses K-Means’ weaknesses:</p>
<ul>
<li>No need to specify number of clusters</li>
<li>Can find clusters of arbitrary shapes</li>
<li>Can identify points that don’t belong to any cluster (points don’t have to belong to any cluster, label = <code>-1</code>)</li>
<li>Initialization is not a problem</li>
</ul></li>
<li><p>Comparison to K-means:</p>
<ul>
<li>does not have to assign all points to clusters</li>
<li>no <code>predict</code> method unlike K-means</li>
<li>non-parametric</li>
</ul></li>
<li><p>Other Con: Needs tuning of 2 non-trivial hyperparameters:</p>
<ul>
<li><code>eps</code> (default=0.5): maximum distance between two samples for one to be considered as in the neighborhood of the other.</li>
<li><code>min_samples</code> (default = 5): number of samples in a neighborhood for a point to be considered as a core point</li>
</ul></li>
<li><p><strong>DBSCAN Failure Cases</strong>: Different densities of clusters</p></li>
</ul>
<section id="how-dbscan-works" class="level3">
<h3 class="anchored" data-anchor-id="how-dbscan-works">How DBSCAN works</h3>
<ul>
<li><p><strong>Kinds of points</strong>:</p>
<ul>
<li><strong>Core point</strong>: A point that has at least <code>min_samples</code> points within <code>eps</code> of it</li>
<li><strong>Border point</strong>: A point that is within <code>eps</code> of a core point, but has less than <code>min_samples</code> points within <code>eps</code> of it</li>
<li><strong>Noise point</strong>: A point that is neither a core point nor a border point</li>
</ul></li>
<li><p><strong>Algorithm</strong>:</p>
<ul>
<li>randomly pick a point that has not been visited</li>
<li>Check if it’s a core point
<ul>
<li>See <code>eps</code> distance around the point if there are <code>min_samples</code> points</li>
</ul></li>
<li>If yes, start a cluster around this point</li>
<li>Check if neighbors are core points and repeat</li>
<li>Once no more core points, pick another random point and repeat</li>
</ul></li>
</ul>
</section>
</section>
<section id="hierarchical-clustering" class="level2">
<h2 class="anchored" data-anchor-id="hierarchical-clustering">Hierarchical Clustering</h2>
<ul>
<li>Hard to decide how many clusters
<ul>
<li>So get complete picture of similarity between points then decide</li>
</ul></li>
<li><strong>Main idea</strong>:
<ul>
<li>Start with each point as a cluster</li>
<li>Merge the closest clusters</li>
<li>Repeat until only a single cluster remains (n-1 steps)</li>
</ul></li>
<li>Visualized as a <code>dendrogram</code></li>
</ul>
<section id="dendrogram" class="level3">
<h3 class="anchored" data-anchor-id="dendrogram">Dendrogram</h3>
<ul>
<li><strong>x-axis</strong>: data points</li>
<li><strong>y-axis</strong>: distance between clusters</li>
<li>Is a tree like plot
<ul>
<li>New parent node for each 2 clusters that are merged</li>
</ul></li>
<li>Length of the vertical line at the point of merging is the distance between the clusters</li>
</ul>
</section>
<section id="linkage-criteria" class="level3">
<h3 class="anchored" data-anchor-id="linkage-criteria">Linkage Criteria</h3>
<ul>
<li>Linkage Criteria determines how to find similarity between clusters</li>
<li><strong>Single Linkage</strong>:
<ul>
<li>Merge smallest <strong>min</strong> distance between points in two clusters</li>
<li>Can lead to chaining</li>
</ul></li>
<li><strong>Complete Linkage</strong>:
<ul>
<li>Merge smallest <strong>max</strong> distance between points in two clusters</li>
<li>Can lead to crowding (tight clusters)</li>
</ul></li>
<li><strong>Average Linkage</strong>:
<ul>
<li>Merge smallest <strong>mean</strong> distance between points in two clusters</li>
</ul></li>
<li><strong>Ward’s Method</strong>:
<ul>
<li>Minimizes the variance of the clusters being merged</li>
<li>Leads to equally sized clusters</li>
</ul></li>
</ul>
</section>
<section id="simplifying-the-dendrogram" class="level3">
<h3 class="anchored" data-anchor-id="simplifying-the-dendrogram">Simplifying the Dendrogram</h3>
<ol type="1">
<li><p><strong>Truncation</strong>:</p>
<ul>
<li><p><code>scipy.cluster.hierarchy.dendrogram</code> has a <code>truncate_mode</code> parameter</p></li>
<li><p>Two levels:</p>
<ul>
<li><code>lastp</code>: show last p merged clusters (only p nodes are shown)</li>
<li><code>level</code>: level is all nodes with <code>p</code> merges from the root</li>
</ul></li>
</ul></li>
<li><p><strong>Flatten</strong></p>
<ul>
<li>Directly cut the dendogram at certain condition (e.g.&nbsp;distance or max number of clusters)</li>
</ul></li>
</ol>
</section>
</section>
<section id="principal-component-analysis-pca" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<section id="dimensionality-reduction-motivation" class="level3">
<h3 class="anchored" data-anchor-id="dimensionality-reduction-motivation">Dimensionality Reduction Motivation</h3>
<ul>
<li><strong>Curse of Dimensionality</strong>
<ul>
<li>As the number of dimensions increases, the volume of the space increases so fast that the available data become sparse + computational complexity increases</li>
</ul></li>
<li><strong>Data Visualization</strong>
<ul>
<li>It is difficult to visualize data in high dimensions</li>
</ul></li>
</ul>
</section>
<section id="pca-overview" class="level3">
<h3 class="anchored" data-anchor-id="pca-overview">PCA Overview</h3>
<ul>
<li><strong>Goal</strong>: Find a low-dimensional representation of the data that captures as much of the variance as possible</li>
<li><strong>Approach</strong>
<ul>
<li>Find the lower dimension hyperplane that minimizes the reconstruction error</li>
<li>Model is the best-fit hyperplane</li>
</ul></li>
</ul>
</section>
<section id="pca-terminology" class="level3">
<h3 class="anchored" data-anchor-id="pca-terminology">PCA Terminology</h3>
<p><span class="math display">\[X = ZW\]</span></p>
<p><span class="math display">\[(n \times d) = (n \times k) \cdot (k \times d)\]</span></p>
<ul>
<li><span class="math inline">\(X\)</span>: original data, (<span class="math inline">\(n \times d\)</span>)</li>
<li><span class="math inline">\(Z\)</span>: coordinates in the lower dimension, (<span class="math inline">\(n \times k\)</span>)</li>
<li><span class="math inline">\(W\)</span>: lower dimension hyperplane, (<span class="math inline">\(k \times d\)</span>)
<ul>
<li><span class="math inline">\(W\)</span> is the principal components</li>
<li>Rows of <span class="math inline">\(W\)</span> are orthogonal to each other</li>
</ul></li>
</ul>
<p>Can <strong>reconstruct</strong> the original data with some error:</p>
<p><span class="math display">\[\hat{X} = ZW\]</span></p>
<p><em>Note</em>: if <span class="math inline">\(k = d\)</span>, then <span class="math inline">\(Z\)</span> is not necessarily <span class="math inline">\(X\)</span> but <span class="math inline">\(\hat{X} = X\)</span> (Frobenius norm)</p>
<ul>
<li><strong>Objective/ Loss Function</strong>:
<ul>
<li>Minimize reconstruction error <span class="math inline">\(\|ZW - X\|_F^2\)</span>
<ul>
<li>Frobeinus norm <span class="math inline">\(||A||_F = \sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2}\)</span></li>
</ul></li>
<li>NOT the same as least squares
<ul>
<li>LS is vertical distance, PCA is orthogonal distance</li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="pca-math" class="level3">
<h3 class="anchored" data-anchor-id="pca-math">PCA Math</h3>
<ul>
<li><p>Singular Value Decomposition (SVD)</p>
<p><span class="math display">\[A = USV^T\]</span></p>
<ul>
<li><span class="math inline">\(A\)</span>: original data, (<span class="math inline">\(n \times d\)</span>)</li>
<li><span class="math inline">\(U\)</span>: <strong>left singular vectors</strong>, (<span class="math inline">\(n \times n\)</span>)
<ul>
<li>orthonormal columns <span class="math inline">\(U_i^TU_j = 0\)</span> for all <span class="math inline">\(i \neq j\)</span></li>
</ul></li>
<li><span class="math inline">\(S\)</span>: <strong>diagonal matrix</strong> of singular values, (<span class="math inline">\(n \times d\)</span>)
<ul>
<li>square root of <strong>eigenvalues</strong> of <span class="math inline">\(A^TA\)</span> or <span class="math inline">\(AA^T\)</span></li>
<li>corresponds to the variance of the data along the principal components (in decreasing order)</li>
</ul></li>
<li><span class="math inline">\(V\)</span>: <strong>right singular vectors</strong>, (<span class="math inline">\(d \times d\)</span>)
<ul>
<li>orthonormal columns (<strong>eigenvectors</strong>)</li>
<li>principal components (get the first <span class="math inline">\(k\)</span> columns)</li>
</ul></li>
</ul></li>
<li><p>For dimensionality reduction, we can use the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(U\)</span> and the first <span class="math inline">\(k\)</span> rows of <span class="math inline">\(V^T\)</span> (equal to first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(V\)</span>)</p></li>
<li><p><strong>PCA Algorithm</strong></p>
<ol type="1">
<li>Center the data (subtract the mean of each column)</li>
<li>Compute the SVD of the centered data to get the principal components <span class="math inline">\(W\)</span>.
<ul>
<li><span class="math inline">\(W\)</span> is the first <span class="math inline">\(k\)</span> columns of <span class="math inline">\(V\)</span></li>
</ul></li>
<li>Variance of each PC is the square of the singular value <span class="math inline">\(s_i^2\)</span></li>
<li>Drop the PCs with the smallest variance</li>
</ol></li>
<li><p><strong>Uniquess of PCA</strong></p>
<ul>
<li>PCA are not unique, similar to eigenvectors</li>
<li>Can add constraints to make it closer to unique:
<ul>
<li>Normalization: <span class="math inline">\(||w_i|| = 1\)</span></li>
<li>Orthogonality: <span class="math inline">\(w_i^Tw_j = 0\)</span> for all <span class="math inline">\(i \neq j\)</span></li>
<li>Sequential PCA: <span class="math inline">\(w_1^Tw_2 = 0\)</span>, <span class="math inline">\(w_2^Tw_3 = 0\)</span>, etc.</li>
</ul></li>
<li>The principal components are unique up to sign</li>
</ul></li>
</ul>
</section>
<section id="choosing-number-of-components" class="level3">
<h3 class="anchored" data-anchor-id="choosing-number-of-components">Choosing Number of Components</h3>
<ul>
<li>No definitive rule</li>
<li>Can look at:
<ul>
<li>Explained variance ratio</li>
<li>Reconstructions plot</li>
</ul></li>
</ul>
</section>
<section id="pca-and-multicollinearity" class="level3">
<h3 class="anchored" data-anchor-id="pca-and-multicollinearity">PCA and Multicollinearity</h3>
<ul>
<li>PCA can be used to remove multicollinearity</li>
<li><strong>Concept</strong>: the principal components are orthogonal to each other so they should not be correlated</li>
</ul>
</section>
<section id="pca-applications" class="level3">
<h3 class="anchored" data-anchor-id="pca-applications">PCA Applications</h3>
<ol type="1">
<li><strong>Data Compression</strong></li>
<li><strong>Feature Extraction</strong></li>
<li><strong>Visualization of High-Dimensional Data</strong></li>
<li><strong>Dimensionality Reduction</strong></li>
<li><strong>Anomaly Detection</strong>
<ul>
<li>Can use the reconstruction error to detect anomalies/ outliers (if the error is too large)</li>
<li>Outliers = high reconstruction error</li>
</ul></li>
</ol>
</section>
<section id="k-means-and-pca" class="level3">
<h3 class="anchored" data-anchor-id="k-means-and-pca">K-means and PCA</h3>
<ul>
<li>PCA is a generalization of K-means</li>
<li>K-means is a special case of PCA where the principal components are the cluster centers</li>
<li>K-means each example is expressed with only one component (one-hot encoding) but in PCA it is a linear combination of all components</li>
</ul>
</section>
</section>
<section id="nmf-non-negative-matrix-factorization" class="level2">
<h2 class="anchored" data-anchor-id="nmf-non-negative-matrix-factorization">NMF (Non-Negative Matrix Factorization)</h2>
<ul>
<li>Useful for when data is created with several independent sources
<ul>
<li>e.g.&nbsp;music with different instruments</li>
</ul></li>
<li><strong>Properties</strong>:
<ul>
<li>Coefficients and basis vectors (components) are <strong>non-negative</strong>
<ul>
<li>Unlike in PCA you can subtract, e.g.&nbsp;<span class="math inline">\(X_i = 14W_0 - 2W_2\)</span></li>
<li>Since cannot cancel out =&gt; <strong>more interpretable</strong></li>
</ul></li>
<li>Will get <strong>different results for different number of components</strong>
<ul>
<li><code>n_components=2</code> will point at extreme, <code>n_components=1</code> will point at mean</li>
<li>Unlike PCA, where first component points at the direction of maximum variance regardless of the number of components</li>
</ul></li>
<li>Slower than PCA</li>
</ul></li>
</ul>
</section>
<section id="comparison-of-pca-lsa-and-nmf" class="level2">
<h2 class="anchored" data-anchor-id="comparison-of-pca-lsa-and-nmf">Comparison of PCA, LSA, and NMF</h2>
<table class="table">
<colgroup>
<col style="width: 7%">
<col style="width: 37%">
<col style="width: 26%">
<col style="width: 28%">
</colgroup>
<thead>
<tr class="header">
<th>Differenciating feature</th>
<th>PCA</th>
<th>NMF</th>
<th>LSA</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Primary use</strong></td>
<td>General dimensionality reduction, visualization, noise reduction.</td>
<td>Part-based decomposition, collaborative filtering, topic modeling.</td>
<td>Topic modeling, document similarity, information retrieval.</td>
</tr>
<tr class="even">
<td><strong>Data types/constraints</strong></td>
<td>Any numerical data.</td>
<td>Non-negative data (e.g., images, text counts), Non-negativity on both features and coefficients.</td>
<td>Text data (term-document matrices), typically applied to non-negative data (e.g., term frequencies).</td>
</tr>
<tr class="odd">
<td><strong>Output components</strong></td>
<td>Principal components that are orthogonal. (sorted by variance)</td>
<td>Basis vectors with non-negative entries only. (not sorted by variance)</td>
<td>Singular vectors representing latent semantic dimensions. (not orthogonal nor sorted)</td>
</tr>
<tr class="even">
<td><strong>Interpretability</strong></td>
<td>Can look at the linear combinations of features. Often difficult to interpret components directly because of the cancellation effects.</td>
<td>Interpretability is better, due to non-negative and additive nature of components.</td>
<td>Moderate</td>
</tr>
</tbody>
</table>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>